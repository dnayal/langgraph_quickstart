{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = \"__GOOGLE_KEY__\"\n",
    "llm = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"Bonjour ! Je suis un professeur de français extrêmement intelligent.  À votre service. Comment puis-je vous aider aujourd'hui ?\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-e6c3ac97-94d8-44a5-afa0-23b809d1197d-0', usage_metadata={'input_tokens': 22, 'output_tokens': 27, 'total_tokens': 49, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "messages = [\n",
    "    SystemMessage(content=\"You are an extremely smart frnech professor that only responds in French.\"),\n",
    "    HumanMessage(content=\"Hello, who are you?\"),\n",
    "]\n",
    "llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AIMessage(content=\"I have no memory of past conversations. Each interaction with me starts fresh.  If you'd like me to answer a question, please ask it again.\\n\", additional_kwargs={}, response_metadata={'prompt_feedback': {'block_reason': 0, 'safety_ratings': []}, 'finish_reason': 'STOP', 'safety_ratings': []}, id='run-19656e07-d42d-4bda-b318-6179a11b9027-0', usage_metadata={'input_tokens': 7, 'output_tokens': 33, 'total_tokens': 40, 'input_token_details': {'cache_read': 0}})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llm.invoke(\"What was my last question?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import PromptTemplate\n",
    "\n",
    "query_template = \"Tell me about {book_name} by {author}\"\n",
    "prompt = PromptTemplate(input_variables=[\"book_name\", \"author\"], template=query_template)\n",
    "\n",
    "chain = prompt | llm\n",
    "print(chain.invoke({\"book_name\":\"Sapiens\", \"author\":\"Yuval Noah Harari\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.agent_toolkits.load_tools import load_tools\n",
    "\n",
    "tools = load_tools([\"arxiv\"])\n",
    "print(tools[0].invoke(\"Attention is all you need\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.tools import WikipediaQueryRun\n",
    "from langchain_community.utilities import WikipediaAPIWrapper\n",
    "from langchain_community.tools import YouTubeSearchTool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to get complete sentences only from a text\n",
    "def full_sentences(text: str) -> str:\n",
    "    last_punctuation = max(text.rfind('.'), text.rfind('?'), text.rfind('!'))\n",
    "    \n",
    "    # If no sentence-ending punctuation is found, return an empty string\n",
    "    if last_punctuation == -1:\n",
    "        return \"\"\n",
    "    \n",
    "    # Return the substring up to and including the last punctuation\n",
    "    return text[:last_punctuation + 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_api_wrapper = WikipediaAPIWrapper(top_k_results=1, doc_content_chars_max=250)\n",
    "wikipedia = WikipediaQueryRun(description=\"A tool to explain things in text format. Use this tool if you think the user’s asked concept is best explained through text.\", api_wrapper=wiki_api_wrapper)\n",
    "wiki_response = full_sentences(wikipedia.invoke(\"Mobius Strip\"))\n",
    "print(wiki_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "youtube = YouTubeSearchTool(description=\"A tool to search YouTube videos. Use this tool if you think the user’s asked concept can be best explained by watching a video.\")\n",
    "print(youtube.run(\"explain mobius strip\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "tools = [wikipedia, youtube]\n",
    "llm_with_tools = llm.bind_tools(tools)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#response = llm_with_tools.invoke([HumanMessage(\"I want to watch latest news on German elections\")])\n",
    "response = llm_with_tools.invoke([HumanMessage(\"When was the great pyramid of Giza made?\")])\n",
    "print(response.content)\n",
    "print(\"=========\")\n",
    "print(response.tool_calls)\n",
    "if (len(response.tool_calls) > 0):\n",
    "    item = response.tool_calls[0]\n",
    "    print(item[\"name\"])\n",
    "    print(item[\"args\"][\"query\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langgraph.prebuilt import create_react_agent\n",
    "system_prompt = SystemMessage(\n",
    "    \"\"\"\n",
    "    You are a helpful bot named Super Commando Dhruv. Your task is to explain topics \n",
    "    asked by the user via two mediums: text or video.\n",
    "    \n",
    "    If the asked topic is best explained via combination of text and video, \n",
    "    then provide a summary of the topic using Wikipedia tool, \n",
    "    along with video links on the topic using Youtube tool.\n",
    "    However, if the asked topic is best explained only in text format, use the Wikipedia tool;\n",
    "    and if only video is the best medium to explain the topic, conduct a YouTube search on it\n",
    "    and return found video links.    \n",
    "   \"\"\"\n",
    ")\n",
    "agent = create_react_agent(llm, tools, state_modifier=system_prompt)\n",
    "agent.invoke({\"messages\" : [HumanMessage(\"What's up?\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke({\"messages\":[HumanMessage(\"How do LLMs work?\")]})\n",
    "print(response[\"messages\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = agent.invoke({\"messages\":[HumanMessage(\"Which year did Titanic sink? I just want to know the year - nothing more.\")]})\n",
    "print(response[\"messages\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'name': 'multiply', 'args': {'a': 3.0, 'b': 2.0}, 'id': '8704ca11-4363-46a7-809c-104c44926e2d', 'type': 'tool_call'}, {'name': 'add', 'args': {'b': 11.0, 'a': 10.0}, 'id': 'dfa69703-48ec-447c-830b-3d18f24c811e', 'type': 'tool_call'}]\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import tool\n",
    "\n",
    "@tool\n",
    "def add(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Tool to add two integers\n",
    "    \n",
    "    Arguments:\n",
    "    a: first integer\n",
    "    b: second integer\n",
    "    \"\"\"\n",
    "    return a + b\n",
    "\n",
    "@tool\n",
    "def multiply(a: int, b: int) -> int:\n",
    "    \"\"\"\n",
    "    Tool to multiply two integers\n",
    "\n",
    "    Arguments:\n",
    "    a: first integer\n",
    "    b: second integer\n",
    "    \"\"\"\n",
    "    return a * b\n",
    "\n",
    "custom_tools = [add, multiply]\n",
    "llm_with_custom_tools = llm.bind_tools(custom_tools)\n",
    "\n",
    "messages = [HumanMessage(\"what is 3 times 2?  And what if you add 10 to 11?\")]\n",
    "ai_msg = llm_with_custom_tools.invoke(messages)\n",
    "print(ai_msg.tool_calls)\n",
    "messages.append(ai_msg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "multiply\n",
      "Tool to multiply two integers\n",
      "\n",
      "Arguments:\n",
      "a: first integer\n",
      "b: second integer\n",
      "{'a': {'title': 'A', 'type': 'integer'}, 'b': {'title': 'B', 'type': 'integer'}}\n"
     ]
    }
   ],
   "source": [
    "print(multiply.name)\n",
    "print(multiply.description)\n",
    "print(multiply.args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for tool_call in ai_msg.tool_calls:\n",
    "    print(tool_call)\n",
    "    print(\"==========\")\n",
    "    selected_tool = {\"add\":add, \"multiply\":multiply}[tool_call[\"name\"].lower()]\n",
    "    tool_msg = selected_tool.invoke(tool_call)\n",
    "    messages.append(tool_msg)\n",
    "\n",
    "messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm_with_custom_tools.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = create_react_agent(llm, custom_tools)\n",
    "agent.invoke({\"messages\":[HumanMessage(\"what is 3 times 2?  And what if you add 10 to 11?\")]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "name='find_max' description='Returns maximum of the two numbers\\n    Arguments:\\n    a: first integer\\n    b: second integer' args_schema=<class 'langchain_core.utils.pydantic.find_max'> func=<function find_max at 0x316147c40> coroutine=<function a_find_max at 0x316147380>\n",
      "2\n",
      "20\n"
     ]
    }
   ],
   "source": [
    "from langchain_core.tools import StructuredTool\n",
    "\n",
    "def find_max(a: int, b: int) -> int:\n",
    "    \"\"\"Returns maximum of the two numbers\n",
    "    Arguments:\n",
    "    a: first integer\n",
    "    b: second integer\n",
    "    \"\"\"\n",
    "    return max(a, b)\n",
    "\n",
    "async def a_find_max(a: int, b: int) -> int:\n",
    "    \"\"\"Returns maximum of the two numbers asynchronously\n",
    "    Arguments:\n",
    "    a: first integer\n",
    "    b: second integer\n",
    "    \"\"\"\n",
    "    return max(a, b)\n",
    "\n",
    "highest_num = StructuredTool.from_function(func=find_max, coroutine=a_find_max)\n",
    "print(highest_num)\n",
    "print(highest_num.invoke({\"a\":1, \"b\":2}))\n",
    "print(await highest_num.ainvoke({\"a\":10,\"b\":20}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using smaller LLMs: SmolLM2 / Llamma 1B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.mps.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "from langchain_huggingface import HuggingFacePipeline\n",
    "\n",
    "# model_name = \"HuggingFaceTB/SmolLM2-1.7B-Instruct\"\n",
    "model_name = \"meta-llama/Llama-3.2-1B-Instruct\"\n",
    "device = \"cpu\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)\n",
    "pipe = pipeline(\"text-generation\", model = model, tokenizer=tokenizer, max_new_tokens=500, temperature=0)\n",
    "hf_llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_llm.invoke([HumanMessage(\"What is the capital of France?\")])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#system_prompt_rewrite = \"You are an AI writing assistant. Your task is to rewrite the user's email to make it more professional and approachable while maintaining its main points and key message. Do not return any text other than the rewritten message.\"\n",
    "#user_prompt_rewrite = \"Rewrite the message below to make it more friendly and approachable while maintaining its main points and key message. Do not add any new information or return any text other than the rewritten message\\nThe message:\"\n",
    "#messages = [{\"role\": \"system\", \"content\": system_prompt_rewrite}, {\"role\": \"user\", \"content\":f\"{user_prompt_rewrite} The CI is failing after your last commit!\"}]\n",
    "messages = [{\"role\": \"user\", \"content\": \"Write a 100 word essay on Transformer LLM archiecture.\"}]\n",
    "\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=100, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install transformers\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "checkpoint = \"HuggingFaceTB/SmolLM-1.7B-Instruct\"\n",
    "\n",
    "device = \"mps\" # for GPU usage or \"cpu\" for CPU usage\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "# for multiple GPUs install accelerate and do `model = AutoModelForCausalLM.from_pretrained(checkpoint, device_map=\"auto\")`\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).to(device)\n",
    "\n",
    "messages = [{\"role\": \"user\", \"content\": \"Write a 100 word essay on Transformer LLM archiecture.\"}]\n",
    "input_text=tokenizer.apply_chat_template(messages, tokenize=False)\n",
    "print(input_text)\n",
    "inputs = tokenizer.encode(input_text, return_tensors=\"pt\").to(device)\n",
    "outputs = model.generate(inputs, max_new_tokens=50, temperature=0.2, top_p=0.9, do_sample=True)\n",
    "print(tokenizer.decode(outputs[0]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
